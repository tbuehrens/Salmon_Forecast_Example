---
title: Steelhead Forecast
author: Thomas Buehrens and Jan Ohlberger
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
params:
  system: "" # system name, or blank
editor_options: 
  chunk_output_type: console
---

<script>
   $(document).ready(function() {
     $head = $('#header');
     $head.prepend('<img src=\"https://privatelands.wdfw.wa.gov/wdfwlogo_clrnotxt.png"\" style=\"float: right;width: 150px;\"/>')
   });
</script>

***

Last Updated `r format(Sys.time(), '%m/%d/%Y')`.

***

# Overview
This script fits various time series models to salmon run size data with or without regression covariates. These models are used to generate one-year-ahead forecasts by splitting the data into "training" and "validation" sets to evaluate the ability of each model to make one-year-ahead forecasts. Forecasts for the last few years of the available time series are evaluated and models compared based on their forecasting performance. Furthermore, weighted ensembles of multiple models are developed by evaluating optimal model weights to construct the best possible ensemble, which is then used to generate the final one-year-ahead forecast of future run size. 

The code repository used to generate this page and complete analyses can be found here: [**(link)**](https://github.com/tbuehrens/Salmon_Forecast_Example)


# Setup
All analyses require R software [**(link)**](https://cran.r-project.org/) (v3.4.3) for data retrieval, data processing, and summarizing model results. First, we configure R to perform our analysis and generate our outputs.
```{r set_options,echo=TRUE,message=FALSE}
options(width = 100)
knitr::opts_chunk$set(message=FALSE)
set.seed(123)
```
Here we will load & install the required packages (needs internet connection if packages have not already been installed).
```{r load_packages,message=FALSE,warning=FALSE,results="hide"}

pkgs<-c("tidyverse","forecast","mgcv","ggplot2","MASS","RColorBrewer","kableExtra","gtools","lubridate","brms","dataRetrieval","ncdf4","reshape2","here","pracma","tidymodels","modeltime","timetk","conflicted","janitor","readr")

if(length(setdiff(pkgs,rownames(installed.packages())))>0){ install.packages(setdiff(pkgs,rownames(installed.packages())),dependencies=T)}
invisible(lapply(pkgs,library,character.only=T))

tidymodels_prefer()
conflict_prefer("lag","dplyr")
conflict_prefer("select","dplyr")
conflict_prefer("rdirichlet","brms")

```
We also need to load helper functions from the functions folder.
```{r load_funcs,message=FALSE,warning=FALSE,results="hide"}

setwd(here("code"))
wd_functions<-"../functions"
sapply(FUN = source, paste(wd_functions, list.files(wd_functions), sep="/"))

```
# Set parameters
Here we specify parameters for our forecasting and evaluation: number of years forward to forecast (only FY=1 currently available), the model-averaging weighting exponent, and the performance metric used to evaluate forecast model performance. Available performance metrics are the	Mean Absolute Percent Error (MAPE), Root Mean Squared Error (RMSE), and Median Symmetric Accuracy (MSA).
```{r set_parameters,message=FALSE,warning=FALSE,results="hide"}
FY <- 1 # number of years foreward to forecast (note: if forecasting more than one year,performance is still evaluated based on one-year ahead forecasts)
k <- 1 # this is the model-averaging weighting exponent--default is 1, larger numbers will tend to more heavily weight "best" models over lower ranked models
stack_metric <- "MSA" # this is the performance measure that will be optimized to find the best stack weights
incl_brms<-"no" # include brms models? 'yes' or 'no'
```

# Load fish data
Here we will load and format our data for analysis. First, we will load and format our fish data for analysis. We can forecast run size or escapement. At the beginning of this section information on the system to forecast is taken from the input parameters and the respective stream gage is selected. Currently available are Willapa Bay and the Chehalis, Hoh, Humptulips, Queets, Quillayute, and Quinault rivers.
```{r load_data,message=FALSE,warning=FALSE,results="show"}
#=========================#
#get salmon abundance data 
#=========================#
systems<-c("Cheh","Hoh","Hump","Quee","Quil","Quin","Will")
gages<-c(12031000,12041200,12039500,12040500,12043000,12039500,12013500)
## using stream gage on Quinault River for nearby Humptulips River

system<-params$system
system_name<-substr(system,1,4)

# data_dir<-"C:/Users/ohlj1477/OneDrive - Washington State Executive Branch Agencies/Population Projection Model/River_Files"
# esc_file<-paste0(data_dir,"/",system_name,"_Esc.csv")
# catch_file<-paste0(data_dir,"/",system_name,"_Catch.csv")
# run_dat<-read_csv(esc_file,col_types="n")%>%
#   left_join(read_csv(catch_file,col_types="n"))%>%
#   rowwise()%>% 
#   mutate(catch=sum(c_across(-c(year,escapement)),na.rm=T)) %>%
#   mutate(across(everything(),round))%>%
#   mutate(runsize_obs=catch+escapement)%>%
#   rename(Year=year)%>%
#   dplyr::select(Year,escapement,catch,runsize_obs)

run_dat<-read_csv("../data/steelhead_run_sizes.csv",col_types="n") %>%
  dplyr::select(Year,all_of(system)) %>%
    rename(runsize_obs=system)

Yrlist<-data.frame(Year=c(min(run_dat$Year):(max(run_dat$Year)+FY)))
thisYr<-as.numeric(format(Sys.Date(),"%Y"))

run_dat<-run_dat %>% right_join(Yrlist)
    
#look at our data  
print(tail(data.frame(run_dat)))
```

---
subtitle: "`r system`"
---

# Load covariate data
Here we will download covariate data from the web (or load from the data folder) that could inform the run size forecasts. Covariates used in this script are stream flow during freshwater residence (minimum, mean, maximum annual flow using stream gages), sea surface temperatures during ocean residence (mean summer and winter SST using ERSST_V5), and the North Pacific Gyre Oscillation (NPGO) index during ocean residence. Other covariate data can easily be added in this section. 
```{r get_covariate_data,message=FALSE,warning=FALSE,results="hide"}
if(FY==1){
#=============#
#get flow data
#=============#

flow_site<-gages[which(systems==system_name)] 

## average daily flow
flow_raw<-readNWISuv(siteNumbers=flow_site,
                     parameterCd="00060", ## code '00060' for discharge
                     startDate=paste0(min(run_dat$Year),"-01-01"),
                     endDate=paste0(max(run_dat$Year),"-12-31"))%>%
  renameNWISColumns()%>%
  mutate(Date=substr(dateTime,1,10))%>% 
  mutate(Year=year(Date),Month=month(Date),Day=day(Date))%>%
  group_by(Year,Month,Day)%>%
  dplyr::summarise(CFS_day=mean(Flow_Inst)) 
  
## minimum, mean, and maximum flow during June this year to May next year
## lag-2 would be year prior to migration to sea of .1+ steelhead 
flow<-flow_raw%>%  
  mutate(Year=ifelse(Month>5,Year+1,Year))%>%
  group_by(Year)%>%
  dplyr::summarise(CFS_min=min(CFS_day),CFS_mean=mean(CFS_day),CFS_max=max(CFS_day))%>%
  dplyr::mutate(CFS_min=log(CFS_min),CFS_mean=log(CFS_mean),CFS_max=log(CFS_max))

#=============#
#get NPGO data
#=============#
NPGO<-read_table("http://www.o3d.org/npgo/npgo.php",skip=29,col_names=F,comment="#")%>%
  filter(!is.na(X2))%>%
  dplyr::rename(Year=X1,Month=X2,NPGO=X3)%>%
  mutate(Year=as.numeric(Year))%>%
  group_by(Year)%>%
  add_tally()%>%
  #filter(!Month>6)%>% #use only spring (Jan-June) NPGO
  filter(!n<12)%>% #use only complete years
  group_by(Year)%>%
  dplyr::summarise(NPGO=mean(NPGO))

#===============#
# Get ERSST data
#===============#
#data.dir<-"C:/Users/buehrtwb/OneDrive - Washington State Executive Branch Agencies/Documents"
data.dir<-"../data/"
sstdat<-get_ersst_v5_data_V2(years=c(1960,thisYr),data.dir=data.dir ,ncfilename="sst.mnmean.nc",latrange=c(46,49),lonrange=c(-124,-127))

sstdat<-sstdat%>%
  mutate(sstdiff=c(NA,diff(resid)))

## select SST metric (mean, gmean, resid)
ssta<-sstdat%>%
  dplyr::select(year,month,meanSST)%>%
  mutate(month = paste0("m_",month))%>%
  rename(Year = year)%>%
  pivot_wider(names_from=month,values_from=meanSST) 

##================#
##Plot ERSST data
##================#
## time series for each month (warmest: July/Aug/Sept and coldest: Jan/Feb/Mar)
# colors<-c(brewer.pal(name="Spectral",n=8), brewer.pal(name="Dark2",n=8))
# ggplot(sstdat,aes(x=factor(year),y=meanSST,group=month,color=month))+
#   scale_color_manual(values=colors)+
#   geom_line()+
#   theme(axis.text.x=element_text(angle=90))

} ## end if(FY==1) statement
```
If we loaded/downloaded covariate data for use in forecasting fish abundances, here we will merge these data with our fish data. The remainder of the sections and the functions used therein rely on this data format.
```{r merge_data,message=FALSE,warning=FALSE,results="hide"}
#================================#
# Merge salmon and covariate data
#================================#
# only use covariates for 1-year-ahead forecasts 
if(FY==1){ 
dat<-run_dat%>%
  left_join(flow)%>%
  left_join(NPGO)%>%
  left_join(ssta)%>%
  data.frame()%>%
  mutate(
    ## large-scale climate index
    NPGO_lag1=scale(lag(NPGO,1)),
    NPGO_lag2=scale(lag(NPGO,2)),
    ## sea surface temperatures
    SST_win_lag1=scale(lag(m_01,1)+lag(m_02,1)+lag(m_03,1)/3),
    SST_win_lag2=scale(lag(m_01,2)+lag(m_02,2)+lag(m_03,2)/3),
    SST_sum_lag1=scale(lag(m_07,1)+lag(m_08,1)+lag(m_09,1)/3),
    SST_sum_lag2=scale(lag(m_07,2)+lag(m_08,2)+lag(m_09,2)/3),
    ## stream flow metrics (cubic feet per second)
    CFS_min_lag2=scale(lag(CFS_min,2)),
    CFS_min_lag3=scale(lag(CFS_min,3)),
    CFS_mean_lag2=scale(lag(CFS_mean,2)),
    CFS_mean_lag3=scale(lag(CFS_mean,3)),
    CFS_max_lag2=scale(lag(CFS_max,2)),
    CFS_max_lag3=scale(lag(CFS_max,3))
    ) %>%
  data.frame() 

## select covariates (drop same year values which can't be used in forecasts)
dat<-dplyr::select(dat,-starts_with("m_"),-CFS_max,-CFS_min,-CFS_mean,-NPGO) 

## variables to include in analysis
all_vars<-names(dat[,-c(1:4)]) ## all of the above

## select covariates
dat<-dat%>%dplyr::select(Year,runsize_obs,all_of(all_vars))

# trim run size NAs, except final forecast year
dat<-data.frame(dat)%>%filter(Year==max(dat$Year) | !is.na(runsize_obs))

} else {
dat<-data.frame(dat)%>%
  dplyr::select(Year,runsize_obs)%>%
  filter(Year>=thisYr | !is.na(runsize_obs))
}

#look at our data  
print(tail(data.frame(dat)))
```

# Select covariates
Here we select the best combination of covariates. We allow a maximum of three covariates in any one model, to avoid long run times and to avoid over-fitting of the data.
```{r select_covariates,message=FALSE,warning=FALSE,results="show"}

#=================================#
# Select covariates and make table
#=================================#
# metrics: stream flow, SST, NPGO
if(FY==1){
dat_all<-dat

## drop  NAs
dat<-dat%>%filter(if_all(.cols=all_of(all_vars),.fns=~!is.na(.x))) 
DY<-dim(dat)[1] # all years 
yr_ind<-c((DY-FY+1):DY) # years to forecast
TY<-DY-12 # training years

## covariate selection
covar_selected<-select_covariates(all_vars,dat)
covar_used<-covar_selected$covars
cavar_table<-data.frame(covars=covar_used)%>%
  mutate(Description = case_when(
    startsWith(covars,"CFS_min") ~ "Minimum stream flow",
    startsWith(covars,"CFS_mean") ~ "Mean stream flow",
    startsWith(covars,"CFS_max") ~ "Maximum stream flow",
    startsWith(covars,"SST_win") ~ "Winter SST",
    startsWith(covars,"SST_spr") ~ "Spring SST",
    startsWith(covars,"SST_sum") ~ "Summer SST",
    startsWith(covars,"SST_fal") ~ "Fall SST",
    startsWith(covars,"NPGO") ~ "NPGO"
    ))%>%
  mutate(Lag = case_when(
    endsWith(covars,"lag1") ~ "1 year prior",
    endsWith(covars,"lag2") ~ "2 years prior",
    endsWith(covars,"lag3") ~ "3 years prior"
    ))

## make covariate table
cavar_table%>%
  kbl(caption = "Table 1. Covariates included in the models",digits =0)%>%
  kable_classic(full_width = F, html_font = "Cambria")

} ## end if(FY==1) statement
```

# Fit forecast models
Here we fit the individual time series models, which include ARIMA models, log-normal Generalized Additive Models (GAM), the Prophet model, Prophet boost, an elastic net model using Lasso and Ridge regression penalties, an error-trend-season model with exponential smoothing, and random forests. In addition, we also compare these models to a long-term mean as predicted value and last years observation as predicted value. 

```{r fit_forecasts,message=FALSE,warning=FALSE,results="show"}

dat<-dat_all%>%dplyr::select(Year,runsize_obs,all_of(covar_used))
dat<-dat%>%filter(if_all(.cols=all_of(covar_used),.fns=~!is.na(.x))) 

DY<-dim(dat)[1] # all years 
yr_ind<-c((DY-FY+1):DY) # years to forecast
TY<-DY-12 # training years

#=============#
# ARIMA models
#=============#
# ARIMA(p,d,q) where p,d,q are AR order, degree of differencing, and MA order
# orders tested: up to 3rd order AR, 2nd order MA, and 1st order differencing
# include ARIMA(0,0,0) to test whether time series are a white noise process

# or define ARIMA orders to be tested and run all combinations
arima_orders<-list(
  c(0,0,0),c(1,0,0),c(0,1,0),c(0,0,1),c(1,1,0),c(1,0,1),c(0,1,1),c(1,1,1),
  c(2,0,0),c(2,0,1),c(2,0,2),c(2,1,0),c(2,1,1),c(2,1,2),
  c(0,0,2),c(0,1,2),c(1,0,2),c(1,1,2),
  c(3,0,0),c(3,0,1),c(3,0,2),c(3,1,0),c(3,1,1),c(3,1,2)
  )
norders<-length(arima_orders)

ARIMAs<-c(NULL)
AICcs<-data.frame(Name=NA,AICc=NA)

# fit models with covariates
if(FY==1){
for(i in 1:norders){
  order<-arima_orders[[i]]
  ARIMA_withcov<-tsCV2(y=log(dat$runsize_obs[-yr_ind]),
            Year=dat$Year,
            xreg=dat%>%dplyr::select(all_of(covar_used))%>%as.matrix(),
            h=FY,
            min=TY,
            type="ARIMA",
            order=order
            )
  
  ARIMA_name<-paste0("ARIMA",paste0(as.numeric(order),collapse=""),"_withcov")
  AICcs[i,1]<-ARIMA_name
  assign(ARIMA_name,ARIMA_withcov)
  ARIMAs[i]<-ARIMA_name
  AICcs[i,2]<-ARIMA_withcov$fit$aicc
} ## end loop over orders
} ## end if(FY==1) statement

# fit models without covariates
for(i in 1:norders){
    order<-arima_orders[[i]]
    try(ARIMA_mod<-tsCV2(y=log(dat$runsize_obs[-yr_ind]),
            Year=dat$Year,
            xreg=NULL,
            h=FY,
            min=TY,
            type="ARIMA",
            order=order
            )
    )
    if(isTRUE(class(ARIMA_mod)=="try-error")) { next } else { 
    ARIMA_name<-paste0("ARIMA",paste0(as.numeric(order),collapse=""))
    AICcs[norders+i,1]<-ARIMA_name
    assign(ARIMA_name,ARIMA_mod)
    ARIMAs[norders+i]<-ARIMA_name
    AICcs[norders+i,2]<-ARIMA_mod$fit$aicc
    }
} ## end loop over orders

## continue with top ten ARIMA models
n_best_models<-10
ARIMAs<-compact(ARIMAs)
AICcs$delong_term_mean_modelAICc<-AICcs$AICc-min(AICcs$AICc)
best_models<-AICcs%>%arrange(desc(-delong_term_mean_modelAICc))%>%slice(1:n_best_models)
ARIMAs<-ARIMAs[ARIMAs%in%best_models$Name]

#====================================#
# Last year observe as predicted value
#====================================#
last_year_obs<-list()
last_year_obs$results<-data.frame(Year=dat$Year,Estimate=c(NA,log(dat$runsize_obs[-yr_ind])))%>%mutate(Estimate=replace(Estimate,which(Year<min(dat$Year)+TY),NA))

#====================#
# Moving average model
#====================#
# simple moving average models up to lag-6
# movavgs<-c(NULL)
# for(i in 2:6){
#   movavg_mod<-list()
#   movavg_mod$results<-data.frame(Year=dat$Year,Estimate=c(NA,movavg(log(dat$runsize_obs[-yr_ind]),n=i,type="s")))%>%mutate(Estimate=replace(Estimate,which(Year<min(dat$Year)+TY),NA))
#   mod_name<-paste0("moving_avg_lag",paste0(as.numeric(i),collapse=""))
#   assign(mod_name,movavg_mod)
#   movavgs[i-1]<-mod_name
# } ## end loop over orders

#===================================#
# Long-term average as predicted value
#====================================#
long_term_mean<-list()
long_term_mean$results<-data.frame(Year=dat$Year,Estimate=rep(mean(log(dat$runsize_obs[-yr_ind])),dim(dat)[1]))%>%mutate(Estimate=replace(Estimate,which(Year<min(dat$Year)+TY),NA))

#===========================================================================#
# Prophet, prophet boost, random forest, elastic net, and error-trend-season
#===========================================================================#

prophet_model<-fit_prophet(dat,TY,yr_ind) # basic FB prophet model
random_forest<-fit_rforest(dat,TY,yr_ind) # 1000 trees?
prophet_boost<-fit_pboost(dat,TY,yr_ind) # takes the longest to fit
elastic_net<-fit_glmnet(dat,TY,yr_ind) # similar to Bayesian horse shoe prior
ets_model<-fit_ets(dat,TY,yr_ind) # error-trend-season / exponential smoothing

#===========#
# GAM models
#===========#
gam<-tsCV2(y=log(dat$runsize_obs[-yr_ind]),
            Year=dat$Year,
            xreg=NULL,
            h=FY,
            min=TY,
            type="gam",
            m=1,
            knots=-1
)

# if(FY==1){
# LFO10<-tsCV2(y=log(dat$runsize_obs[-yr_ind]),
#             Year=dat$Year,
#             xreg=dat%>%dplyr::select(all_of(covar_used))%>%as.matrix(),
#             h=FY,
#             min=TY,
#             type="gam",
#             m=1,
#             knots=-1
# ) 
# }

#================================#
# BRMS models with Horseshoe Prior
#================================#
if(incl_brms=="yes"){
  gp_horseshoe<-fc3(
    Year=dat$Year,
    TY=TY
    ,vars=vars
    ,dat=dat
    ,formula = as.formula(paste0("log(runsize_obs) ~ gp(Year) + ",paste(vars,collapse = "+")))
    ,prior = set_prior(horseshoe(df = 1, par_ratio = 0.5), class="b")
  )
  if(FY==1){
    LFO12<-fc3(
      Year=dat$Year,
      TY=TY
      ,vars=vars
      ,dat=dat
      ,formula=as.formula(paste0("log(runsize_obs) ~ arma(p=1,q=1,cov=TRUE) + ", paste(vars,collapse = "+")))
      ,prior = set_prior(horseshoe(df = 1, par_ratio = 0.5), class="b")
    )
  }
}
```

# Summarize Forecasts 
Here we will summarize our forecasts and plot observed and forecasted runsize for all models.
```{r summarize_forecasts,message=FALSE,warning=FALSE,results="asis"}

#===================#
# Summarize Forecasts
#===================#
models<-c(ARIMAs,"prophet_model","random_forest","prophet_boost","elastic_net", "ets_model","gam","long_term_mean","last_year_obs")#,movavgs)

if(incl_brms=="yes"){ models<-c(models,"gp_horseshoe","LFO12") }

forecasts<-summarize_forecasts(models) 

#========================================#
# Table of forecast results for final year
#========================================#
forecast_years<-dat$Year[c((DY-FY+1):(DY))]

# forecasts%>%
#   group_by(Model)%>%
#   filter(Year%in%forecast_years)%>%
#   #filter(Year==max(Year,na.rm=T))%>%
#   kbl(caption = "Table 2. Forecasts for final year of data.",digits =0)%>%
#   kable_classic(full_width = F, html_font = "Cambria")

#===========================#
# Plot of final year forecast
#===========================#
colors<-brewer.pal(name="Spectral",n=8)
colors<-colorRampPalette(colors)(length(unique(forecasts$Model)))
ggplot(data=forecasts,aes(x=Year,y=Estimate,color=Model))+
  labs(title="One-Year-Ahead Forecasts")+
  geom_line(size=1)+
  scale_color_manual(values=colors)+
  geom_point(data=dat,mapping=aes(x=Year,y=runsize_obs),size=1,color="black")+
  ylim(0,NA)+
  theme_bw()

```

# Evaluate forecast performance
Here we will evaluate forecasts using a few performance metrics: Mean Absolute Percent Error (MAPE), Root Mean Squared Error (RMSE), and Median Symmetric Accuracy (MSA). RMSE and MAPE tend to more heavily penalize over-forecasts relative to under-forecasts for abundance (because errors are right-skewed), whereas MSA is specifically tailored for right tailed distributions.
```{r evaluate_forecast_skill,message=FALSE,warning=FALSE,results="show"}
forecast_skills_models<-evaluate_forecasts(forecasts=forecasts%>%
                    filter(Year>min(Year)),observations=dat)

#===============================
# Table of model forecast skills
#===============================
forecast_skills_models%>%
  kbl(caption = "Table 3.Forecast Performance based on full set of one-year-ahead forecasts.",digits =3)%>%
  kable_classic(full_width = F, html_font = "Cambria")
```

# Evaluate ensembles
Here we will calculate model weights. For the ensembles, we calculate leave-forward-out performance year by year in order to develop weighted ensembles (with recalculated weights each year). We then evaluate performance of these one-year-ahead ensembles and compare it with the performance of the original models.
```{r evaluate_forecasts,message=FALSE,warning=FALSE,results="show"}

#==============================
# evaluate forecast performance
#==============================
# not models without uncertainty
forecasts<-forecasts%>%
  filter(!grepl("long_term_mean",Model))%>%
  filter(!grepl("last_year_obs",Model))%>%
  filter(!grepl("moving_avg",Model)) 

forecast_results<-evaluate_all_forecasts(forecasts=forecasts,observations=dat)

#=================================
# Table of final year model weights
#=================================
forecast_results$final_model_weights%>%
  dplyr::select(Model,RMSE_weight,MAPE_weight,MSA_weight, Stacking_weight)%>%
  kbl(caption = "Table 4. Final Year Model Weights based on full dataset of one-year-ahead performance.",digits =3)%>%
  kable_classic(full_width = F, html_font = "Cambria")

#========================
# Table of Forecast Skill
#========================
forecast_results$forecast_skill%>%
  kbl(caption = "Table 5.Forecast Performance based on full set of one-year-ahead forecasts including ensembles.",digits =3)%>%
  kable_classic(full_width = F, html_font = "Cambria")

#=======================================
# Table of final year ensemble forecasts
#=======================================
# forecast_results$ensembles%>%
#   group_by(Model)%>%
#   filter(Year==max(Year,na.rm=T))%>%
#   kbl(caption = "Table 4. Ensemble forecasts one year ahead of final year of data.",digits =0)%>%
#   kable_classic(full_width = F, html_font = "Cambria")

#========================
# Plot ensemble forecasts
#========================
# ggplot(data=forecast_results$ensembles,aes(x=Year,y=Estimate,color=Model))+
#   labs(title="One-Year-Ahead Ensemble Forecasts")+
#   geom_line(size=1)+
#   scale_color_brewer(palette="Spectral")+
#   geom_point(data=dat,mapping=aes(x=Year,y=runsize_obs),size=2,color="black")+
#   ylim(0,NA)+
#   theme_bw()

```

# Best model
Finally, we identify the best model and plot and summarize its results.
```{r get_best_model,message=FALSE,warning=FALSE,results="show"}

best_model <- get_best_model(
  forecasts = forecasts
  ,ensembles = forecast_results$ensembles
  ,stack_metric = stack_metric
  ,forecast_skill = forecast_results$forecast_skill
)
if(length(unique(best_model$Model))==2) { best_model<-best_model%>%filter(Model=="Stack_weighted") }

#================
# Plot Best Model 
#================
ggplot(data=best_model,aes(x=Year,y=Estimate,color=Model))+
  geom_ribbon(aes(ymin=L95,ymax=U95,fill=Model),color=NA,alpha=0.4)+
  facet_wrap(~Model,ncol=1)+
  labs(title=paste0("Best Model One-Year-Ahead Forecasts: ",system))+
  geom_line(size=1)+
  scale_color_brewer(palette="Spectral")+
  geom_point(data=dat,mapping=aes(x=Year,y=runsize_obs),size=2,color="black")+
  ylim(0,NA)+
  theme_bw()

best_mu<-log(best_model%>%
               filter(Year==max(Year))%>%
               dplyr::select(Estimate))%>%unlist()

best_sd<-best_model%>%
  filter(Year==max(Year))%>%
  dplyr::select(sd)%>%unlist()

best_draws=data.frame(Estimate=rlnorm(25000,best_mu,best_sd[1]))

## save plot for report
pdf(paste0("../output/Best-model-",system,".pdf"),width=5,height=4)
par(mar=c(3,3,2,1),mgp=c(2,0.5,0),tck=-0.02,cex.axis=0.8,cex.lab=1)
tab<-best_model %>% dplyr::select(Year,Estimate,L95,U95)
xlim<-c(min(dat_all$Year),max(dat_all$Year))
ylim<-c(0,1.3*max(c(dat_all$runsize_obs,tab$U95),na.rm=TRUE))
plot(NA,NA,xlim=xlim,ylim=ylim,xlab="Year",ylab="Run size",pch=16,cex=1.2)
col<-"firebrick"
polyx<-c(tab$Year,rev(tab$Year))
polyy<-c(tab$L95,rev(tab$U95))
polygon(polyx,polyy,lwd=0.5,col=alpha(col,0.5),border=NA)
lines(tab$Year,tab$Estimate,lwd=2,col=alpha(col,1))
points(dat_all$Year,dat_all$runsize_obs,pch=16,cex=1,col="black")
txt<-paste0("Run forecast: ",system)
mtext(txt,side=3,line=0.5,cex=1,font=1,adj=0)
l<-c("Historical run size","1-year-ahead forecasts\n(median and 95% CI)")
legend("topright",legend=l,pch=NA,col=c(NA,alpha(col,0.5)),lwd=c(NA,8),cex=0.8,seg.len=1.5, bty="n")
legend("topright",legend=l,pch=c(16,NA),col=c("black",col),lwd=c(NA,2),cex=0.8,seg.len=1.5, bty="n")
dev.off()

write.csv(tab,paste0("../output/Best-model-",system,".csv"))

write.csv(dat_all,paste0("../output/Data-observed-",system,".csv"))

#==========================
# Best model final year pdf
#==========================
# ggplot(data=best_draws,aes(x=Estimate,color=NA),alpha=0.4)+
#   geom_density(aes(fill="Estimate"))+
#   xlim(0,quantile(best_draws$Estimate,0.995))+
#   labs(title="Best Model Forecast")+
#   scale_color_brewer(palette="Spectral")+
#   ylim(0,NA)+
#   theme_bw()

#============================
# Best Model final year table
#============================
best_model%>%
  group_by(Model)%>%
  filter(Year%in%forecast_years)%>%
  dplyr::select(Model,Year,Estimate,L95,U95)%>%
  kbl(caption = "Table 6. Best one year ahead model forecasts for the final year.",digits =0)%>%
  kable_classic(full_width = F, html_font = "Cambria")

best_forecast<-best_model%>%
  group_by(Model)%>%
  filter(Year%in%forecast_years)%>%
  dplyr::select(Model,Year,Estimate,L95,U95)

write.csv(best_forecast,paste0("../output/",system,"_forecast.csv"))

```
